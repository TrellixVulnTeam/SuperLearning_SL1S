{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 9: Implementing Text Mining\n",
    "\n",
    "#### The problems in this assignment are based on the exercises of Chapter 20 in Data Mining for Business Analytics.\n",
    "\n",
    "#### Scenario Consider the case of a website that caters to the needs of a specific farming community, and carries classified ads intended for that community. Anyone, including robots, can post an ad via a web interface, and the site owners have problems with ads that are fraudulent, spam, or simply not relevant to the community. They have provided a file with 4143 ads, each ad in a row, and each ad labeled as either −1 (not relevant) or 1 (relevant). The goal is to develop a predictive model that can classify ads automatically.\n",
    "\n",
    "#### Data For more information about the data set see https://archive.ics.uci.edu/ml/datasets/Farm+Ads. Each ad includes the words on the ad creative and the words from the landing page. Each word from the creative is given a prefix of 'ad-'. Title ('title-') and header ('header-') HTML markups are noted in a similar way in the text of the landing page. Stemming and stop word removal is already applied.\n",
    "\n",
    "#### Data preprocessing Open the file farm-ads.csv, and brieﬂy review some of the relevant and non-relevant ads to get a ﬂavor for their contents.\n",
    "\n",
    "#### Following the example in the chapter, preprocess the data in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Cobbadmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Required Packages\n",
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.snowball import EnglishStemmer \n",
    "import matplotlib.pylab as plt\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>ad-abdominal ad-aortic ad-aneurysm ad-doctorfinder ad-help ad-patient ad-local ad-physician ad-treat ad-aaa ad-www ad-findtheaaanswer ad-org page found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-abdominal ad-aortic ad-aneurysm ad-million...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-absorbent ad-oil ad-snar ad-factory ad-dir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-acid ad-reflux ad-relief ad-top ad-treatme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-acid ad-reflux ad-symptom ad-acid ad-reflu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-addiction ad-treatment ad-detox ad-opioid ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-adenocarcinoma ad-treatment ad-learn ad-le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-adhd ad-sign ad-top ad-ten ad-adhd ad-symp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-adhd ad-treatment ad-adult ad-information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-adorable ad-farm ad-mural ad-easy ad-paint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>ad-adult ad-hepatitis ad-recently ad-diagnose...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   -1  \\\n",
       "0  -1   \n",
       "1  -1   \n",
       "2  -1   \n",
       "3  -1   \n",
       "4  -1   \n",
       "5  -1   \n",
       "6  -1   \n",
       "7  -1   \n",
       "8  -1   \n",
       "9  -1   \n",
       "\n",
       "   ad-abdominal ad-aortic ad-aneurysm ad-doctorfinder ad-help ad-patient ad-local ad-physician ad-treat ad-aaa ad-www ad-findtheaaanswer ad-org page found  \n",
       "0   ad-abdominal ad-aortic ad-aneurysm ad-million...                                                                                                        \n",
       "1   ad-absorbent ad-oil ad-snar ad-factory ad-dir...                                                                                                        \n",
       "2   ad-acid ad-reflux ad-relief ad-top ad-treatme...                                                                                                        \n",
       "3   ad-acid ad-reflux ad-symptom ad-acid ad-reflu...                                                                                                        \n",
       "4   ad-addiction ad-treatment ad-detox ad-opioid ...                                                                                                        \n",
       "5   ad-adenocarcinoma ad-treatment ad-learn ad-le...                                                                                                        \n",
       "6   ad-adhd ad-sign ad-top ad-ten ad-adhd ad-symp...                                                                                                        \n",
       "7   ad-adhd ad-treatment ad-adult ad-information ...                                                                                                        \n",
       "8   ad-adorable ad-farm ad-mural ad-easy ad-paint...                                                                                                        \n",
       "9   ad-adult ad-hepatitis ad-recently ad-diagnose...                                                                                                        "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "farm_ads_df = pd.read_csv('farm-ads.csv')\n",
    "farm_ads_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>-1</th>\n",
       "      <th>ad-abdominal ad-aortic ad-aneurysm ad-doctorfinder ad-help ad-patient ad-local ad-physician ad-treat ad-aaa ad-www ad-findtheaaanswer ad-org page found</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-yorkie ad-puppy ad-sale ad-toy ad-teacup a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4133</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-yorkie ad-puppy ad-sale ad-yorkie ad-toy a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4134</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zealand ad-travel ad-plan ad-zealand ad-tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4135</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zupreem ad-pet ad-food ad-price ad-guarant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4136</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zupreem ad-pet ad-food ad-price ad-guarant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4137</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zupreem ad-pet ad-food ad-price ad-guarant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4138</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zupreem ad-pet ad-food ad-zupreem ad-anima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4139</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zupreem ad-pet ad-food ad-zupreem ad-anima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4140</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zupreem ad-pet ad-food ad-zupreem ad-anima...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4141</th>\n",
       "      <td>1</td>\n",
       "      <td>ad-zupreem ad-pet ad-food ad-zupreem ad-anima...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      -1  \\\n",
       "4132   1   \n",
       "4133   1   \n",
       "4134   1   \n",
       "4135   1   \n",
       "4136   1   \n",
       "4137   1   \n",
       "4138   1   \n",
       "4139   1   \n",
       "4140   1   \n",
       "4141   1   \n",
       "\n",
       "      ad-abdominal ad-aortic ad-aneurysm ad-doctorfinder ad-help ad-patient ad-local ad-physician ad-treat ad-aaa ad-www ad-findtheaaanswer ad-org page found  \n",
       "4132   ad-yorkie ad-puppy ad-sale ad-toy ad-teacup a...                                                                                                        \n",
       "4133   ad-yorkie ad-puppy ad-sale ad-yorkie ad-toy a...                                                                                                        \n",
       "4134   ad-zealand ad-travel ad-plan ad-zealand ad-tr...                                                                                                        \n",
       "4135   ad-zupreem ad-pet ad-food ad-price ad-guarant...                                                                                                        \n",
       "4136   ad-zupreem ad-pet ad-food ad-price ad-guarant...                                                                                                        \n",
       "4137   ad-zupreem ad-pet ad-food ad-price ad-guarant...                                                                                                        \n",
       "4138   ad-zupreem ad-pet ad-food ad-zupreem ad-anima...                                                                                                        \n",
       "4139   ad-zupreem ad-pet ad-food ad-zupreem ad-anima...                                                                                                        \n",
       "4140   ad-zupreem ad-pet ad-food ad-zupreem ad-anima...                                                                                                        \n",
       "4141   ad-zupreem ad-pet ad-food ad-zupreem ad-anima...                                                                                                        "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "farm_ads_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (5 points) Based on the analysis of the document, create a term-document matrix. Examine the term-document matrix. Is it sparse or dense? Look at the first row of the term-document matrix and determine the meaning of the non-zero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 S1  S2\n",
      "aaa               0   1\n",
      "abdominal         0   1\n",
      "ad                0  13\n",
      "aneurysm          0   1\n",
      "aortic            0   1\n",
      "doctorfinder      0   1\n",
      "findtheaaanswer   0   1\n",
      "found             0   1\n",
      "help              0   1\n",
      "local             0   1\n",
      "org               0   1\n",
      "page              0   1\n",
      "patient           0   1\n",
      "physician         0   1\n",
      "treat             0   1\n",
      "www               0   1\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(token_pattern='[a-zA-Z!:)]+')\n",
    "counts = count_vect.fit_transform(farm_ads_df)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The term-document matrix is sparse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##### Looking at the first row of the term-document matrix, the non-zero elements are the counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (8 points) Using logistic regression, partition the data (60% training, 40% validation, random_state=1), and develop a model to classify the documents as ‘relevant’ or ‘non-relevant.’ Comment on its efficacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 180 of 318 stopwords\n",
      "a            about        above        across       after        afterwards   \n",
      "again        against      all          almost       alone        along        \n",
      "already      also         although     always       am           among        \n",
      "amongst      amoungst     amount       an           and          another      \n",
      "any          anyhow       anyone       anything     anyway       anywhere     \n",
      "are          around       as           at           back         be           \n",
      "became       because      become       becomes      becoming     been         \n",
      "before       beforehand   behind       being        below        beside       \n",
      "besides      between      beyond       bill         both         bottom       \n",
      "but          by           call         can          cannot       cant         \n",
      "co           con          could        couldnt      cry          de           \n",
      "describe     detail       do           done         down         due          \n",
      "during       each         eg           eight        either       eleven       \n",
      "else         elsewhere    empty        enough       etc          even         \n",
      "ever         every        everyone     everything   everywhere   except       \n",
      "few          fifteen      fifty        fill         find         fire         \n",
      "first        five         for          former       formerly     forty        \n",
      "found        four         from         front        full         further      \n",
      "get          give         go           had          has          hasnt        \n",
      "have         he           hence        her          here         hereafter    \n",
      "hereby       herein       hereupon     hers         herself      him          \n",
      "himself      his          how          however      hundred      i            \n",
      "ie           if           in           inc          indeed       interest     \n",
      "into         is           it           its          itself       keep         \n",
      "last         latter       latterly     least        less         ltd          \n",
      "made         many         may          me           meanwhile    might        \n",
      "mill         mine         more         moreover     most         mostly       \n",
      "move         much         must         my           myself       name         \n",
      "namely       neither      never        nevertheless next         nine         \n",
      "no           nobody       none         noone        nor          not          \n"
     ]
    }
   ],
   "source": [
    "stopWords = list(sorted(ENGLISH_STOP_WORDS))\n",
    "ncolumns = 6; nrows= 30\n",
    "\n",
    "print('First {} of {} stopwords'.format(ncolumns * nrows, len(stopWords)))\n",
    "for i in range(0, len(stopWords[:(ncolumns * nrows)]), ncolumns):\n",
    "    print(''.join(word.ljust(13) for word in stopWords[i:(i+ncolumns)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      S1  S2\n",
      "page   0   1\n"
     ]
    }
   ],
   "source": [
    "# Create a custom tokenizer that will use NLTK for tokenizing and lemmatizing \n",
    "# (removes interpunctuation and stop words)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "counts = count_vect.fit_transform(farm_ads_df)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  S1         S2\n",
      "aaa              0.0   1.693147\n",
      "abdominal        0.0   1.693147\n",
      "ad               0.0  22.010913\n",
      "aneurysm         0.0   1.693147\n",
      "aortic           0.0   1.693147\n",
      "doctorfinder     0.0   1.693147\n",
      "findtheaaanswer  0.0   1.693147\n",
      "found            0.0   1.693147\n",
      "help             0.0   1.693147\n",
      "local            0.0   1.693147\n",
      "org              0.0   1.693147\n",
      "page             0.0   1.693147\n",
      "patient          0.0   1.693147\n",
      "physician        0.0   1.693147\n",
      "treat            0.0   1.693147\n",
      "www              0.0   1.693147\n"
     ]
    }
   ],
   "source": [
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts = count_vect.fit_transform(farm_ads_df)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-490fbedb4670>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mad\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Step 2: preprocessing (tokenization, stemming, and stopwords)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "corpus=[]\n",
    "#corpus.append[ad.text]\n",
    "for ad in farm_ads_df:\n",
    "        if ad in corpus:\n",
    "            continue\n",
    "        corpus.extend[ad]\n",
    "\n",
    "# Step 2: preprocessing (tokenization, stemming, and stopwords)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(corpus)\n",
    "\n",
    "# Step 3: TF-IDF and latent semantic analysis\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "\n",
    "# Extract 20 concepts using LSA ()\n",
    "svd = TruncatedSVD(20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (3 points) Why use the concept-document matrix, and not the term-document matrix, to provide the predictor variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It appears that one is just the transpose of the other, but the data reduction technique we used (singular value decomposition) reduced the number of columns (documents) but kept the number of rows (words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
