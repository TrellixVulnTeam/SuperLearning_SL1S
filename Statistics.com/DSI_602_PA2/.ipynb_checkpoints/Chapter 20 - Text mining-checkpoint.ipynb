{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20: Text mining\n",
    "\n",
    "> (c) 2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck \n",
    ">\n",
    "> Code included in\n",
    ">\n",
    "> _Data Mining for Business Analytics: Concepts, Techniques, and Applications in Python_ (First Edition) \n",
    "> Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n",
    "\n",
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/peter/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.snowball import EnglishStemmer \n",
    "import matplotlib.pylab as plt\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
    "\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.1 Term-document representation of words in sentences S1-S3\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          S1  S2  S3\n",
      "first      1   0   0\n",
      "here       0   0   1\n",
      "is         1   1   1\n",
      "second     0   1   0\n",
      "sentence   1   1   1\n",
      "the        1   0   1\n",
      "third      0   0   1\n",
      "this       1   1   0\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first sentence.',\n",
    "        'this is a second sentence.',\n",
    "        'the third sentence is here.']\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.2 Term-document representation of words in sentences S1-S4 (Example 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           S1  S2  S3  S4\n",
      "all         0   0   0   1\n",
      "first       1   0   0   0\n",
      "forth       0   0   0   1\n",
      "here        0   0   1   0\n",
      "is          1   1   1   0\n",
      "of          0   0   0   1\n",
      "second      0   1   0   0\n",
      "sentence    1   1   1   0\n",
      "sentences   0   0   0   1\n",
      "the         1   0   1   0\n",
      "third       0   0   1   0\n",
      "this        1   1   0   0\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Learn features based on text. Include special characters that are part of a word in the analysis\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.3 Tokenization of S1-S4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            S1  S2  S3  S4\n",
      ":)           0   1   0   0\n",
      "a            0   1   0   0\n",
      "all          0   0   0   1\n",
      "first        1   0   0   0\n",
      "forth        0   0   0   1\n",
      "here         0   0   1   0\n",
      "is           1   1   1   0\n",
      "of           0   0   0   1\n",
      "second       0   1   0   0\n",
      "sentence     0   1   1   0\n",
      "sentence!!   1   0   0   0\n",
      "sentences    0   0   0   1\n",
      "the          1   0   1   0\n",
      "third        0   0   1   0\n",
      "this         1   1   0   0\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Learn features based on text. Include special characters that are part of a word in the analysis\n",
    "count_vect = CountVectorizer(token_pattern='[a-zA-Z!:)]+')\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.4 Stopwords in scitkit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 180 of 318 stopwords\n",
      "a            about        above        across       after        afterwards   \n",
      "again        against      all          almost       alone        along        \n",
      "already      also         although     always       am           among        \n",
      "amongst      amoungst     amount       an           and          another      \n",
      "any          anyhow       anyone       anything     anyway       anywhere     \n",
      "are          around       as           at           back         be           \n",
      "became       because      become       becomes      becoming     been         \n",
      "before       beforehand   behind       being        below        beside       \n",
      "besides      between      beyond       bill         both         bottom       \n",
      "but          by           call         can          cannot       cant         \n",
      "co           con          could        couldnt      cry          de           \n",
      "describe     detail       do           done         down         due          \n",
      "during       each         eg           eight        either       eleven       \n",
      "else         elsewhere    empty        enough       etc          even         \n",
      "ever         every        everyone     everything   everywhere   except       \n",
      "few          fifteen      fifty        fill         find         fire         \n",
      "first        five         for          former       formerly     forty        \n",
      "found        four         from         front        full         further      \n",
      "get          give         go           had          has          hasnt        \n",
      "have         he           hence        her          here         hereafter    \n",
      "hereby       herein       hereupon     hers         herself      him          \n",
      "himself      his          how          however      hundred      i            \n",
      "ie           if           in           inc          indeed       interest     \n",
      "into         is           it           its          itself       keep         \n",
      "last         latter       latterly     least        less         ltd          \n",
      "made         many         may          me           meanwhile    might        \n",
      "mill         mine         more         moreover     most         mostly       \n",
      "move         much         must         my           myself       name         \n",
      "namely       neither      never        nevertheless next         nine         \n",
      "no           nobody       none         noone        nor          not          \n"
     ]
    }
   ],
   "source": [
    "stopWords = list(sorted(ENGLISH_STOP_WORDS))\n",
    "ncolumns = 6; nrows= 30\n",
    "\n",
    "print('First {} of {} stopwords'.format(ncolumns * nrows, len(stopWords)))\n",
    "for i in range(0, len(stopWords[:(ncolumns * nrows)]), ncolumns):\n",
    "    print(''.join(word.ljust(13) for word in stopWords[i:(i+ncolumns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.5 Text reduction of S1-S4 using stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         S1  S2  S3  S4\n",
      "forth     0   0   0   1\n",
      "second    0   1   0   0\n",
      "sentenc   1   1   1   1\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first     sentence!! ',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Create a custom tokenizer that will use NLTK for tokenizing and lemmatizing \n",
    "# (removes interpunctuation and stop words)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.6 tf-idf matrix for S1-S4 example (after tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 S1        S2        S3        S4\n",
      "all        0.000000  0.000000  0.000000  2.386294\n",
      "first      2.386294  0.000000  0.000000  0.000000\n",
      "forth      0.000000  0.000000  0.000000  2.386294\n",
      "here       0.000000  0.000000  2.386294  0.000000\n",
      "is         1.287682  1.287682  1.287682  0.000000\n",
      "of         0.000000  0.000000  0.000000  2.386294\n",
      "second     0.000000  2.386294  0.000000  0.000000\n",
      "sentence   1.287682  1.287682  1.287682  0.000000\n",
      "sentences  0.000000  0.000000  0.000000  2.386294\n",
      "the        1.693147  0.000000  1.693147  0.000000\n",
      "third      0.000000  0.000000  2.386294  0.000000\n",
      "this       1.693147  1.693147  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "text = ['this is the first     sentence!!',\n",
    "        'this is a second Sentence :)',\n",
    "        'the third sentence, is here ',\n",
    "        'forth of all sentences']\n",
    "\n",
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts = count_vect.fit_transform(text)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.7 Importing and labeling the records, preprocessing text, and producing concept matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import and label records\n",
    "corpus = []\n",
    "label = []\n",
    "with ZipFile('AutoAndElectronics.zip') as rawData:\n",
    "    for info in rawData.infolist():\n",
    "        if info.is_dir(): \n",
    "            continue\n",
    "        label.append(1 if 'rec.autos' in info.filename else 0)\n",
    "        corpus.append(rawData.read(info))\n",
    "\n",
    "# Step 2: preprocessing (tokenization, stemming, and stopwords)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: TF-IDF and latent semantic analysis\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "\n",
    "# Extract 20 concepts using LSA ()\n",
    "svd = TruncatedSVD(20)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 20.8 Fitting a predictive model to the autos and electronics discussion data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix (Accuracy 0.9550)\n",
      "\n",
      "       Prediction\n",
      "Actual   0   1\n",
      "     0 389   8\n",
      "     1  28 375\n"
     ]
    }
   ],
   "source": [
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracty\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(data={'prob': [p[1] for p in logit_reg.predict_proba(Xtest)], 'actual': ytest})\n",
    "df = df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "ax = liftChart(df.actual, labelBars=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
