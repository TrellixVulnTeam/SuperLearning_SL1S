{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenization \n",
    "Consider the following text version of a post to an online learning forum in a statistics course.\n",
    "\n",
    "Thanks John!<br /><br /><font size=\"3\">\n",
    "\n",
    "&quot;Illustrations and demos will be\n",
    "\n",
    "provided for students to work through on\n",
    "\n",
    "their own&quot;</font>.\n",
    "\n",
    "Do we need that to finish project? If yes,\n",
    "\n",
    "where to find the illustration and demos? \n",
    "\n",
    "Thanks for your help.<img title=\"smiles\"\n",
    "\n",
    "alt=\"smiles\" src=\"\\url{http://lms.statistics.\n",
    "\n",
    "com/pix/smartpix.php/statistics_com_1/s/smil\n",
    "\n",
    "ey.gif}\" \\><br /><br />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Cobbadmin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import nltk\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem.snowball import EnglishStemmer \n",
    "import matplotlib.pylab as plt\n",
    "from dmba import printTermDocumentMatrix, classificationSummary, liftChart\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 (3 points) Identify 10 non-word tokens in the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               S1  S2  S3  S4  S5\n",
      "and             0   1   0   1   0\n",
      "be              0   1   0   0   0\n",
      "demos           0   1   0   1   0\n",
      "do              0   0   1   0   0\n",
      "find            0   0   0   1   0\n",
      "finish          0   0   1   0   0\n",
      "for             0   1   0   0   1\n",
      "help            0   0   0   0   1\n",
      "if              0   0   0   1   0\n",
      "illustration    0   0   0   1   0\n",
      "illustrations   0   1   0   0   0\n",
      "john            1   0   0   0   0\n",
      "need            0   0   1   0   0\n",
      "on              0   1   0   0   0\n",
      "own             0   1   0   0   0\n",
      "project         0   0   1   0   0\n",
      "provided        0   1   0   0   0\n",
      "smiles          0   0   0   0   1\n",
      "students        0   1   0   0   0\n",
      "thanks          1   0   0   0   1\n",
      "that            0   0   1   0   0\n",
      "the             0   0   0   1   0\n",
      "their           0   1   0   0   0\n",
      "through         0   1   0   0   0\n",
      "to              0   1   1   1   0\n",
      "we              0   0   1   0   0\n",
      "where           0   0   0   1   0\n",
      "will            0   1   0   0   0\n",
      "work            0   1   0   0   0\n",
      "yes             0   0   0   1   0\n",
      "your            0   0   0   0   1\n"
     ]
    }
   ],
   "source": [
    "text = ['Thanks John!',\n",
    "        'Illustrations and demos will be provided for students to work through on their own.',\n",
    "        ' Do we need that to finish project?',\n",
    "        'If yes, where to find the illustration and demos?',\n",
    "        ' Thanks for your help.smiles']\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer()\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 180 of 318 stopwords\n",
      "a            about        above        across       after        afterwards   \n",
      "again        against      all          almost       alone        along        \n",
      "already      also         although     always       am           among        \n",
      "amongst      amoungst     amount       an           and          another      \n",
      "any          anyhow       anyone       anything     anyway       anywhere     \n",
      "are          around       as           at           back         be           \n",
      "became       because      become       becomes      becoming     been         \n",
      "before       beforehand   behind       being        below        beside       \n",
      "besides      between      beyond       bill         both         bottom       \n",
      "but          by           call         can          cannot       cant         \n",
      "co           con          could        couldnt      cry          de           \n",
      "describe     detail       do           done         down         due          \n",
      "during       each         eg           eight        either       eleven       \n",
      "else         elsewhere    empty        enough       etc          even         \n",
      "ever         every        everyone     everything   everywhere   except       \n",
      "few          fifteen      fifty        fill         find         fire         \n",
      "first        five         for          former       formerly     forty        \n",
      "found        four         from         front        full         further      \n",
      "get          give         go           had          has          hasnt        \n",
      "have         he           hence        her          here         hereafter    \n",
      "hereby       herein       hereupon     hers         herself      him          \n",
      "himself      his          how          however      hundred      i            \n",
      "ie           if           in           inc          indeed       interest     \n",
      "into         is           it           its          itself       keep         \n",
      "last         latter       latterly     least        less         ltd          \n",
      "made         many         may          me           meanwhile    might        \n",
      "mill         mine         more         moreover     most         mostly       \n",
      "move         much         must         my           myself       name         \n",
      "namely       neither      never        nevertheless next         nine         \n",
      "no           nobody       none         noone        nor          not          \n"
     ]
    }
   ],
   "source": [
    "stopWords = list(sorted(ENGLISH_STOP_WORDS))\n",
    "ncolumns = 6; nrows= 30\n",
    "\n",
    "print('First {} of {} stopwords'.format(ncolumns * nrows, len(stopWords)))\n",
    "for i in range(0, len(stopWords[:(ncolumns * nrows)]), ncolumns):\n",
    "    print(''.join(word.ljust(13) for word in stopWords[i:(i+ncolumns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-word tokens in the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-word tokens in the passage:\n",
      "         S1  S2  S3  S4  S5\n",
      "demo      0   1   0   1   0\n",
      "finish    0   0   1   0   0\n",
      "illustr   0   1   0   1   0\n",
      "john      1   0   0   0   0\n",
      "need      0   0   1   0   0\n",
      "project   0   0   1   0   0\n",
      "provid    0   1   0   0   0\n",
      "student   0   1   0   0   0\n",
      "thank     1   0   0   0   1\n",
      "work      0   1   0   0   0\n",
      "yes       0   0   0   1   0\n"
     ]
    }
   ],
   "source": [
    "# Create a custom tokenizer that will use NLTK for tokenizing and lemmatizing \n",
    "# (removes interpunctuation and stop words)\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.stemmer = EnglishStemmer()\n",
    "        self.stopWords = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n",
    "                if t.isalpha() and t not in self.stopWords]\n",
    "\n",
    "# Learn features based on text\n",
    "count_vect = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "counts = count_vect.fit_transform(text)\n",
    "\n",
    "print(\"Non-word tokens in the passage:\")\n",
    "printTermDocumentMatrix(count_vect, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2 (2 points) Suppose this passage constitutes a document to be classified, but you are not certain of the business goal of the classification task. Identify material (at least 20% of the terms) that, in your judgment, could be discarded fairly safely without knowing that goal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     S1        S2        S3        S4        S5\n",
      "and            0.000000  1.916291  0.000000  1.916291  0.000000\n",
      "be             0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "demos          0.000000  1.916291  0.000000  1.916291  0.000000\n",
      "do             0.000000  0.000000  2.609438  0.000000  0.000000\n",
      "find           0.000000  0.000000  0.000000  2.609438  0.000000\n",
      "finish         0.000000  0.000000  2.609438  0.000000  0.000000\n",
      "for            0.000000  1.916291  0.000000  0.000000  1.916291\n",
      "help           0.000000  0.000000  0.000000  0.000000  2.609438\n",
      "if             0.000000  0.000000  0.000000  2.609438  0.000000\n",
      "illustration   0.000000  0.000000  0.000000  2.609438  0.000000\n",
      "illustrations  0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "john           2.609438  0.000000  0.000000  0.000000  0.000000\n",
      "need           0.000000  0.000000  2.609438  0.000000  0.000000\n",
      "on             0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "own            0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "project        0.000000  0.000000  2.609438  0.000000  0.000000\n",
      "provided       0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "smiles         0.000000  0.000000  0.000000  0.000000  2.609438\n",
      "students       0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "thanks         1.916291  0.000000  0.000000  0.000000  1.916291\n",
      "that           0.000000  0.000000  2.609438  0.000000  0.000000\n",
      "the            0.000000  0.000000  0.000000  2.609438  0.000000\n",
      "their          0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "through        0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "to             0.000000  1.510826  1.510826  1.510826  0.000000\n",
      "we             0.000000  0.000000  2.609438  0.000000  0.000000\n",
      "where          0.000000  0.000000  0.000000  2.609438  0.000000\n",
      "will           0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "work           0.000000  2.609438  0.000000  0.000000  0.000000\n",
      "yes            0.000000  0.000000  0.000000  2.609438  0.000000\n",
      "your           0.000000  0.000000  0.000000  0.000000  2.609438\n"
     ]
    }
   ],
   "source": [
    "# Apply CountVectorizer and TfidfTransformer sequentially\n",
    "count_vect = CountVectorizer()\n",
    "tfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\n",
    "counts = count_vect.fit_transform(text)\n",
    "tfidf = tfidfTransformer.fit_transform(counts)\n",
    "\n",
    "printTermDocumentMatrix(count_vect, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x31 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 37 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because the TF-IDF identifies documents with frequent occurrences of rare terms (i.e. yields high values for documents with a relatively high frequency for terms that are relatively rare overall, and near-zero values for terms that are absent from a document, or present in most documents) , the items with the high TF-IDF values can be safely discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3 (3 points) Suppose the classification task is to predict whether this post requires the attention of the instructor, or whether a teaching assistant might suffice. Identify the 20% of the terms that you think might be most helpful in that task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8f97f8ac8d7a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;31m# run logistic regression model on training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mlogit_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mlogit_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;31m# print confusion matrix and accuracty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\jenkij\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1317\u001b[0m             raise ValueError(\"This solver needs samples of at least 2 classes\"\n\u001b[0;32m   1318\u001b[0m                              \u001b[1;34m\" in the data, but the data contains only one\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1319\u001b[1;33m                              \" class: %r\" % classes_[0])\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "label = []\n",
    "# with ZipFile(text) as rawData:\n",
    "#    for info in rawData.infolist():\n",
    "#        if info.is_dir():\n",
    "#            continue\n",
    "#        label.append(1 if 'rec.autos' in info.filename else 0)\n",
    "#        corpus.append(rawData.read(info))\n",
    "for word in text:\n",
    "    if word in corpus:\n",
    "        continue\n",
    "    else:\n",
    "        label.append(1 if word in corpus else 0)\n",
    "        corpus.append(word)\n",
    "preprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\n",
    "preprocessedText = preprocessor.fit_transform(corpus)\n",
    "\n",
    "# Step 3: TF-IDF and latent semantic analysis\n",
    "tfidfTransformer = TfidfTransformer()\n",
    "tfidf = tfidfTransformer.fit_transform(preprocessedText)\n",
    "\n",
    "# Extract concepts using LSA ()\n",
    "svd = TruncatedSVD()\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "lsa_tfidf = lsa.fit_transform(tfidf)\n",
    "\n",
    "# split dataset into 60% training and 40% test set\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n",
    "\n",
    "# run logistic regression model on training\n",
    "logit_reg = LogisticRegression(solver='lbfgs')\n",
    "logit_reg.fit(Xtrain, ytrain)\n",
    "\n",
    "# print confusion matrix and accuracty\n",
    "classificationSummary(ytest, logit_reg.predict(Xtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x11 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 14 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4 (3 points) What aspect of the passage is most problematic from the standpoint of simply using a bag-of-words approach, as opposed to an approach in which meaning is extracted?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first sentence of the passage, \"\"Illustrations and demos will be provided for students to work through on their own\" is problematic from the standpoint of simply using a bag-of-words approach because the concepts to which the passage will map terms is not obvious.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
