{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Homework: Loading data\n",
    "\n",
    "Create a new IPython Notebook and use it to answer the following assignments. Note that the **c** sections for each are meant to be a little more challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Load the MovieData.csv dataset as described in this week's lesson, and use it to find the following values:\n",
    "\n",
    "a. What is the average US Gross of movies in the dataset?\n",
    "\n",
    "b. How many movies in the dataset have budgets greater than $20 million?\n",
    "\n",
    "c. How many movies were released by each film distributor? (Hint: this could be a good place to use dictionaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "To solve this problem, first load the data like we did in the lesson itself. Then, figure out what operation you need to take on each row: summing up for (a); checking a condition and then counting for (b), and adding to a dictionary and then counting for (c). Once you've determined that, iterate over all the rows and apply your operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the tools we'll need\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we define the utility functions we're going to use:\n",
    "\n",
    "def convert_to_int(text):\n",
    "    '''\n",
    "    Convert a string to an integer\n",
    "    '''\n",
    "    try:\n",
    "        return int(text)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def make_date(date_str):\n",
    "    '''\n",
    "    Turn a MM/DD/YY string into a datetime object\n",
    "    '''\n",
    "    m, d, y = date_str.split(\"/\")\n",
    "    m = int(m)\n",
    "    d = int(d)\n",
    "    y = int(y)\n",
    "    if y > 13:\n",
    "        y += 1900\n",
    "    else:\n",
    "        y += 2000\n",
    "    return dt.datetime(y, m, d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Release_Date\tMovie\tDistributor\tBudget\tUS Gross\tWorldwide Gross\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now we open the data:\n",
    "f = open(\"MovieData.csv\")\n",
    "data = []\n",
    "print(f.readline()) # Skip the first row\n",
    "for row in f: \n",
    "    row =  row.split(\"\\t\")\n",
    "    row[0] = make_date(row[0])\n",
    "    row[3] = convert_to_int(row[3]) # Budget\n",
    "    row[4] = convert_to_int(row[4]) # US Gross\n",
    "    row[5] = convert_to_int(row[5]) # Worldwide Gross\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) \n",
    "\n",
    "To calculate the average movie gross, we first need to get all the movie grosses. Remember (or check) that the US Gross is in column 4.\n",
    "\n",
    "Note: if you try to add None to a number you'll get an error; you need to check for that, or use try/except, to handle None entries. We also should count only the rows where the gross is present, and not those where it is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44185793.44224795\n"
     ]
    }
   ],
   "source": [
    "total = 0.0 # Make it a float, to make division work\n",
    "count = 0 # Increase by 1 for every row with a gross.\n",
    "for row in data:\n",
    "    try:\n",
    "        total += row[4]\n",
    "        count += 1\n",
    "    except:\n",
    "        pass # Do nothing\n",
    "\n",
    "print(total / count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "This is even simpler; for each row, check the budget and keep a count of how many budgets are greater than 20 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1657\n"
     ]
    }
   ],
   "source": [
    "big_budget = 0 # Counter for how many rows meet our criteria\n",
    "for row in data:\n",
    "    if row[3] > 20000000: # Check the budget size\n",
    "        big_budget += 1 \n",
    "print(big_budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "For this assignment, we need to do two things: keep track of the distributors, and associate a running count with each one. This is exactly the sort of task dictionaries were developed for. Remember that a dict associates some fixed key (in this case, a distributor) with a changeable value (like a count). \n",
    "\n",
    "We can use a single dictionary to track all the distributors. Each row, check whether that distributor is already in the dictionary. If so, add 1 to its value; if not, start the count at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 659\n",
      "Paramount Pictures 258\n",
      "Freestyle 2\n",
      "MGM/UA 137\n",
      "MGM 3\n",
      "Screen Gems 1\n",
      "Dimension Films 2\n",
      "Testimony Pictures 1\n",
      "Walt Disney Pictures 1\n",
      "RKO 2\n",
      "Galaxy 1\n",
      "Orion 17\n",
      "Paladin 1\n",
      "Fine Line 16\n",
      "Shooting Gallery 1\n",
      "Island/Alive 1\n",
      "Cloud Ten Pictures 1\n",
      "Anchor Bay 4\n",
      "Jerry Gross Organization 1\n",
      "Zion 1\n",
      "ART 1\n",
      "Avco Embassy 5\n",
      "Black Diamond Pictures 1\n",
      "Samuel Goldwyn 9\n",
      "Paramount Classics 12\n",
      "RS Entertainment 1\n",
      "Yash Raj 1\n",
      "Sony Classics 78\n",
      "Big Pictures 1\n",
      "Live Entertainment 3\n",
      "Sony Pictures 2\n",
      "Disney 1\n",
      "Off Hollywood Pictures 1\n",
      "Freestyle/Darko 1\n",
      "IFC Films 20\n",
      "Rogue Pictures 1\n",
      "American International Pictures 1\n",
      "Five and Two Pictures 1\n",
      "Magnolia 15\n",
      "New Line 138\n",
      "Videos 1\n",
      "Legacy 1\n",
      "New World Pictures 1\n",
      "Excel Entertainment 5\n",
      "Wellspring 1\n",
      "PION 1\n",
      "Picturehouse 6\n",
      "Bigger Picture 1\n",
      "Outrider Pictures 1\n",
      "Overture Films 1\n",
      "J.F. Prods 1\n",
      "Walt Disney Co. 9\n",
      "Gramercy 12\n",
      "CBS Films 3\n",
      "New Yorker 2\n",
      "Off-Hollywood Distribution 1\n",
      "Weinstein 1\n",
      "Weinstein/Dimension 1\n",
      "Film Sales Co. 1\n",
      "Small Planet 1\n",
      "Monterey Media 1\n",
      "Universal/Arenas Entertainment 1\n",
      "Warner Independent Pictures 7\n",
      "Universal 261\n",
      "Sony/Screen Gems 1\n",
      "Lion's Gate 79\n",
      "CFP 1\n",
      "David Keith Co. 1\n",
      "Overture 6\n",
      "RED HOR 1\n",
      "New World 5\n",
      "Focus Features 36\n",
      "USA Films 15\n",
      "Avatar 1\n",
      "Attitude Films 1\n",
      "Lot 47 1\n",
      "Regent Releasing 1\n",
      "Goldwyn 6\n",
      "Lionsgate 13\n",
      "Lavender House 1\n",
      "Access 1\n",
      "Paramount Vantage 3\n",
      "JeTi Films 1\n",
      "INWOO 1\n",
      "LIONS 1\n",
      "Destination Films 2\n",
      "Vitagraph Films 2\n",
      "Stratosphere 1\n",
      "Palm Pictures 3\n",
      "Magnolia Pictures 1\n",
      "Winstar 1\n",
      "Empire 1\n",
      "Tartan 1\n",
      "Zeitgeist 7\n",
      "Consolidated Pictures Group 1\n",
      "CustomFlix 1\n",
      "Palisades Entertainment 1\n",
      "October 6\n",
      "Roadside 2\n",
      "Triumph 1\n",
      "Sony/Tristar 1\n",
      "Columbia 26\n",
      "Truly Indie 1\n",
      "Trimark 9\n",
      "IDP/Stratosphere 1\n",
      "WinStar 1\n",
      "Universal/Rogue 1\n",
      "NORTH 1\n",
      "Film Foundry 1\n",
      "Buena Vista 227\n",
      "Regent 1\n",
      "Providence 2\n",
      "IDP/Goldwyn 2\n",
      "New Films Int'l 1\n",
      "Strand Releasing 2\n",
      "Painted Zebra Releasing 1\n",
      "Weinstein Ci. 1\n",
      "IDP/Gold Circle 1\n",
      "Film Movement 2\n",
      "Filmways 2\n",
      "OpenEdge Media 1\n",
      "WellSpring 2\n",
      "The Movie Partners 1\n",
      "Polygram Films 5\n",
      "Mulberry Square Releasing 1\n",
      "NEW LTN 1\n",
      "Roadside Attractions 4\n",
      "Third Rail 2\n",
      "Phaedra 2\n",
      "Shotwell Media 1\n",
      "Strand 8\n",
      "Damiano 1\n",
      "Giant 1\n",
      "Music Box 1\n",
      "Good Machine 1\n",
      "Eros 5\n",
      "Artistic License 1\n",
      "Atlantic 1\n",
      "Dimension 28\n",
      "8 X Entertainment 1\n",
      "IDP/Sam Goldwyn 1\n",
      "Savoy 3\n",
      "RAIN 1\n",
      "Independent Artists 1\n",
      "Samuel Goldwyn Films 1\n",
      "Fox Searchlight 65\n",
      "Cannon 4\n",
      "October Films 1\n",
      "Power Point 1\n",
      "Odeon 1\n",
      "FilmDistrict 1\n",
      "Kino 5\n",
      "ThinkFilm 9\n",
      "Rogue 1\n",
      "Senator Films 1\n",
      "TriStar Pictures 4\n",
      "Weintraub 2\n",
      "Romar 1\n",
      "Orion Classics 1\n",
      "Hemdale Film Coorporation 1\n",
      "United Film Distribution 2\n",
      "Warner Bros. 311\n",
      "First Look 9\n",
      "Galactic 1\n",
      "DEJ Productions 1\n",
      "TLA Releasing 1\n",
      "Apparition 4\n",
      "Halestorm Entertainment 1\n",
      "Cowboy 7\n",
      "3D Entertainment 1\n",
      "20th Century Fox 230\n",
      "Destination 4\n",
      "Newmarket Films 1\n",
      "First Independent Pictures 1\n",
      "United Artists 23\n",
      "IDP 2\n",
      "Palm/Manga 1\n",
      "MORO 1\n",
      "Weinstein Co. 33\n",
      "Lorimar 2\n",
      "Miramax 135\n",
      "RBC Radio, LLC 1\n",
      "Sony/Gems 1\n",
      "Artisan 23\n",
      "Halestorm 1\n",
      "Barking Cow 1\n",
      "Lions Gate/IFC Films/Fellowship Adventure Group 1\n",
      "New Century Vista Film Company 1\n",
      "CHRIST 1\n",
      "Relativity 1\n",
      "Fabrication Films 1\n",
      "Cinema con Sabor 1\n",
      "New Market 7\n",
      "Sony 312\n",
      "Island 1\n",
      "Inerstar 1\n",
      "Warner Independent 3\n",
      "DreamWorks SKG 78\n",
      "Embassy 1\n",
      "Summit Entertainment 3\n",
      "Summit 11\n",
      "Roxie Releasing 1\n",
      "Cinema Service 1\n",
      "Lions Gate 1\n",
      "Alliance 4\n",
      "Rainforest Films 1\n",
      "Oscilloscope 2\n",
      "First Look Pictures 1\n",
      "Indican 7\n"
     ]
    }
   ],
   "source": [
    "distributors = {} # Empty dictionary\n",
    "for row in data:\n",
    "    distributor = row[2] # Get the film distributor\n",
    "    # If we've already seen this distributor before:\n",
    "    if distributor in distributors:\n",
    "        distributors[distributor] += 1 \n",
    "    # Otherwise, start a new counter:\n",
    "    else:\n",
    "        distributors[distributor] = 1 \n",
    "\n",
    "# Now print the results:\n",
    "for distributor in distributors:\n",
    "    print(distributor, distributors[distributor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Load the earthquake data in QuakeData.csv, and use it to answer the following questions:\n",
    "\n",
    "a. How many earthquakes are in the dataset?\n",
    "\n",
    "b. What is the average magnitude of earthquakes in the dataset?\n",
    "\n",
    "c. The DateTime format in this dataset is a bit trickier than it was for movies. Try to parse it into a datetime object. Do most earthquakes happen between midnight and noon, or noon to midnight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "This question calls on you to apply the techniques you learned in the lesson to a new dataset, with different columns. The steps will be roughly the same, with some tweaking to fit the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data:\n",
    "\n",
    "Like before, the first thing to do is to look at the first few rows of data and see how it's structured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DateTime,Latitude,Longitude,Depth,Magnitude,MagType,NbStations,Gap,Distance,RMS,Source,EventID,Version\n",
      "\n",
      "2012-01-01T00:30:08.770+00:00,12.008,143.487,35.0,5.1,mb,178,45,,1.20,pde,pde20120101003008770_35,1363392487731\n",
      "\n",
      "2012-01-01T00:43:42.770+00:00,12.014,143.536,35.0,4.4,mb,29,121,,0.98,pde,pde20120101004342770_35,1363392488431\n",
      "\n",
      "2012-01-01T00:50:08.040+00:00,-11.366,166.218,67.5,5.3,mb,143,43,,0.82,pde,pde20120101005008040_67,1363392488479\n",
      "\n",
      "2012-01-01T01:22:07.660+00:00,-6.747,130.008,145.0,4.2,mb,14,112,,1.16,pde,pde20120101012207660_145,1363392488594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open(\"QuakeData.csv\")\n",
    "# Print the top few rows to see the format\n",
    "for i in range(5):\n",
    "    print(f.readline()) # Get the columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we see that the first row contains the column names. Unlike the Movie data, however, the values are separated with commas, not tabs. \n",
    "\n",
    "Since it isn't clear what data type to expect from each column, I'm going to do something a bit different here, and have a function that tries to convert each record to a floating-point number, and returns a string if it fails. I'll also use list comprehension to make the row conversion faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(record):\n",
    "    try:\n",
    "        return float(record)\n",
    "    except:\n",
    "        return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DateTime', 'Latitude', 'Longitude', 'Depth', 'Magnitude', 'MagType', 'NbStations', 'Gap', 'Distance', 'RMS', 'Source', 'EventID', 'Version\\n']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "f = open(\"QuakeData.csv\")\n",
    "print(f.readline().split(\",\")) # Print the column headers\n",
    "for row in f:\n",
    "    row = row.split(\",\") # Split on commas\n",
    "    row = [convert(x) for x in row] # Convert each entry\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "Each row corresponds to a single earthquake, so a count of quakes is just a count of rows -- which is just the length of the *data* list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12684\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "To find the average magnitude, you need to make a list of all the magnitudes and then just average it. Look at the columns, above, and see which one is Magnitude (it's the 5th one, or row index 4). Note that you can use the built-in **sum(...)** function to sum a list of numbers. Divide the sum by the length of the list, making sure not to round it to an integer, and you have the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.558483128350625\n"
     ]
    }
   ],
   "source": [
    "quake_sizes = [row[4] for row in data] # Make a list of all magnitudes\n",
    "total = sum(quake_sizes) * 1.0 # Multiply by 1.0 to make sure it's a float\n",
    "print(total / len(quake_sizes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c)\n",
    "\n",
    "The first step to parsing a date is figuring out its format. The **convert** function that I created above left dates as strings; let's look at some arbitrary date and see how it's formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-04T12:48:42.100+00:00\n"
     ]
    }
   ],
   "source": [
    "test_date = data[100][0] # Some arbitrary record\n",
    "print(test_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it looks like the format is Year-Month-Day, followed by the letter 'T', followed by the time. This might be a good place to use string subsetting, and figure out where in the string each piece of information is. The year is the first four characters, the month is characters 6 and 7 (indices 5,6), etc. Test this out and make sure you've got the right indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012 01 04 12 48 42\n"
     ]
    }
   ],
   "source": [
    "year = test_date[:4]\n",
    "month = test_date[5:7]\n",
    "day = test_date[8:10]\n",
    "hour = test_date[11:13]\n",
    "minute = test_date[14:16]\n",
    "second = test_date[17:19]\n",
    "\n",
    "print(year, month, day, hour, minute, second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can create a function to split the date-time string and turn it into a datetime object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_datetime(record):\n",
    "    year = int(record[:4])\n",
    "    month = int(record[5:7])\n",
    "    day = int(record[8:10])\n",
    "    hour = int(record[11:13])\n",
    "    minute = int(record[14:16])\n",
    "    second = int(record[17:19])\n",
    "    return dt.datetime(year, month, day, hour, minute, second)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2012-01-04 12:48:42\n"
     ]
    }
   ],
   "source": [
    "# Testing the function on our test date:\n",
    "print(make_datetime(test_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use make_datetime to create a list of datetime objects, one for each earthquake. We can then check the hour for each, and count how many happen in each time block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6241\n",
      "6443\n"
     ]
    }
   ],
   "source": [
    "# Now for the real deal:\n",
    "quake_times = [make_datetime(row[0]) for row in data]\n",
    "morning_count = 0\n",
    "evening_count = 0\n",
    "for time in quake_times:\n",
    "    if time.hour < 12: \n",
    "        morning_count += 1\n",
    "    else:\n",
    "        evening_count += 1\n",
    "print(morning_count)\n",
    "print(evening_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "In this question, you're asked to use what you've learned to analyze unstructured data -- a plain text file, *data.txt* containing the Facebook's description of its business in its [mandatory annual report to shareholders](http://investor.fb.com/secfiling.cfm?filingID=1326801-15-6) (called a Form 10K).\n",
    "\n",
    "**Bonus:** *sorted(...)* is a built-in Python function for sorting lists according to some order. For extra credit, figure out how to sort dictionary keys based on the values in descending order, and sort all your answers below. To read more about sorting, start at the Python documentation here:\n",
    "https://docs.python.org/3.5/howto/sorting.html#sortinghowto\n",
    "\n",
    "a) Read in the text in the file, and count how many times each individual word appears. For this exercise, words are separated by whitespace -- don't worry about colons, dashes, etc.\n",
    "\n",
    "b) [Stop words](https://en.wikipedia.org/wiki/Stop_words) are words that appear so often in a language that they aren't useful for analysis (you may have noticed them in your results for **a**). Below is a list of stop words taken from [NLTK](http://www.nltk.org/), the Natural Language Toolkit for Python. Read in the document and count words again -- but this time, convert all the words to lower-case, and only include the words that *aren't* on the stop word list. Also remove any punctuation (the characters .,?!-) from the beginning and end of words Finally, only output the words which appear more than once.\n",
    "\n",
    "c) Write the results of the previous section (including words that appear only once) to a csv file. There should be two columns, one for the word and the other for the count. For example:\n",
    "\n",
    "    Word,Count\n",
    "    million,2\n",
    "    objectives,1\n",
    "    ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STOP_WORDS = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', \n",
    "'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)\n",
    "\n",
    "For this section, we need to load *data.txt*, loop over each row, and separate each row into words. Remember, we can split on whitespace using the *.split()* method. Next, we use a dictionary to count how many times each word appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data.txt\")\n",
    "word_counts = {}\n",
    "for row in f:\n",
    "    for word in row.split():\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the dictionary, we can just loop over the words and print out the word and count for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAUs 1\n",
      "website 1\n",
      "device 1\n",
      "driving 1\n",
      "objectives, 1\n",
      "Android, 2\n",
      "types 1\n",
      "in 8\n",
      "whether 1\n",
      "WhatsApp 1\n",
      "34% 1\n",
      "plugins, 1\n",
      "them 6\n",
      "receive 2\n",
      "similarly 1\n",
      "designed 1\n",
      "within 2\n",
      "including 3\n",
      "what 1\n",
      "Create 3\n",
      "efforts 1\n",
      ", 2\n",
      "Atlas, 1\n",
      "How 3\n",
      "medium-sized 1\n",
      "programming 1\n",
      "right-hand 1\n",
      "everywhere 1\n",
      "computers, 1\n",
      "discovery 1\n",
      "payments 1\n",
      "trusted 1\n",
      "and 42\n",
      "products 1\n",
      "help 6\n",
      "available 3\n",
      "platform's 1\n",
      "campaigns. 1\n",
      "focuses 2\n",
      "connect 1\n",
      "others 1\n",
      "Facebook. 1\n",
      "Our 5\n",
      "active 1\n",
      "open 1\n",
      "set 1\n",
      "who 6\n",
      "improve 1\n",
      "showing 1\n",
      "objectives 1\n",
      "We 10\n",
      "Phone 1\n",
      "engaging 1\n",
      "places 1\n",
      "photos 2\n",
      "then 1\n",
      "Messenger 3\n",
      "such 3\n",
      "compared 2\n",
      "top 1\n",
      "Finally, 1\n",
      "marketers, 2\n",
      "exposure, 1\n",
      "Instagram. 1\n",
      "goods 1\n",
      "Developers 1\n",
      "Audience 3\n",
      "million 2\n",
      "creating 1\n",
      "based 2\n",
      "drive 1\n",
      "developers. 2\n",
      "generate 3\n",
      "websites 1\n",
      "social 1\n",
      "value 2\n",
      "at 1\n",
      "LiveRail. 1\n",
      "goals. 1\n",
      "about 1\n",
      "modifications 1\n",
      "grow 1\n",
      "performance 1\n",
      "(SMS) 1\n",
      "invites, 1\n",
      "throughout 1\n",
      "grow, 1\n",
      "mission 1\n",
      "connected. 1\n",
      "marketing 1\n",
      "chat 1\n",
      "Network, 2\n",
      "Payments 2\n",
      "By 1\n",
      "across 1\n",
      "but 1\n",
      "accessing 1\n",
      "those 2\n",
      "infrastructure 2\n",
      "connected 1\n",
      "distribution 1\n",
      "location, 1\n",
      "providing 3\n",
      "marketers 4\n",
      "addition 1\n",
      "December 4\n",
      "including: 1\n",
      "Use 1\n",
      "can 4\n",
      "engagement. 1\n",
      "ad 6\n",
      "with 8\n",
      "messages 1\n",
      "ads, 1\n",
      "opinions, 1\n",
      "as 4\n",
      "not 1\n",
      "sales, 2\n",
      "development 1\n",
      "brand. 1\n",
      "Feed 1\n",
      "appear 1\n",
      "Value 3\n",
      "want 1\n",
      "app 1\n",
      "them, 1\n",
      "share, 1\n",
      "followers 1\n",
      "In 1\n",
      "businesses, 1\n",
      "going 1\n",
      "online 4\n",
      "requests, 1\n",
      "ideas, 1\n",
      "Facebook's 1\n",
      "have 1\n",
      "best 1\n",
      "how 1\n",
      "it 1\n",
      "customize 1\n",
      "supports 1\n",
      "awareness 1\n",
      "world. 1\n",
      "to 33\n",
      "build 1\n",
      "serving 1\n",
      "sharing, 1\n",
      "News 1\n",
      "in-store 2\n",
      "Phone, 1\n",
      "majority 1\n",
      "purchase 2\n",
      "business 3\n",
      "person 1\n",
      "tools, 1\n",
      "(DAUs) 1\n",
      "Marketers 3\n",
      "selling 1\n",
      "campaign 1\n",
      "the 12\n",
      "secure, 1\n",
      "power 1\n",
      "computers. 5\n",
      "align 1\n",
      "user 1\n",
      "sell 1\n",
      "feed 1\n",
      "well 1\n",
      "engagement 1\n",
      "insights 2\n",
      "build, 1\n",
      "integrates 1\n",
      "marketers. 1\n",
      "2014 2\n",
      "friends 2\n",
      "able 1\n",
      "increase 3\n",
      "using 1\n",
      "more 1\n",
      "multiple 1\n",
      "a 9\n",
      "daily 1\n",
      "monetize 3\n",
      "discover, 1\n",
      "integrate 1\n",
      "all 1\n",
      "provide 1\n",
      "745 1\n",
      "public 1\n",
      "world 2\n",
      "filter 1\n",
      "applications. 2\n",
      "through 1\n",
      "our 7\n",
      "buy 1\n",
      "platforms 1\n",
      "digital 1\n",
      "enable 5\n",
      "audiences 1\n",
      "campaigns 1\n",
      "effects, 1\n",
      "virtual 1\n",
      "These 1\n",
      "devices. 3\n",
      "2013. 1\n",
      "gender, 1\n",
      "instantly 1\n",
      "direct 1\n",
      "Facebook 13\n",
      "people 12\n",
      "technology 1\n",
      "easy-to-use, 1\n",
      "around 1\n",
      "age, 1\n",
      "ads 9\n",
      "application 7\n",
      "messaging, 1\n",
      "Overview 1\n",
      "factors 1\n",
      "achieve 1\n",
      "substantial 1\n",
      "other 3\n",
      "brand, 1\n",
      "create 2\n",
      "results 1\n",
      "people, 1\n",
      "useful 1\n",
      "interfaces 1\n",
      "share 4\n",
      "an 5\n",
      "revenue 4\n",
      "closest 1\n",
      "priority 1\n",
      "reach 2\n",
      "reach. 1\n",
      "placements 1\n",
      "web 4\n",
      "give 1\n",
      "personal 6\n",
      "advertisers 2\n",
      "tools 3\n",
      "take 1\n",
      "us, 1\n",
      "functionality 1\n",
      "both 1\n",
      "friends. 1\n",
      "is 8\n",
      "texting 1\n",
      "activities 1\n",
      "large, 1\n",
      "works 1\n",
      "dynamically 1\n",
      "People 1\n",
      "iOS 1\n",
      "marketers' 1\n",
      "Nokia 1\n",
      "cross-platform 1\n",
      "directly 1\n",
      "seamlessly 1\n",
      "planning 1\n",
      "application. 1\n",
      "determines 1\n",
      "ways 1\n",
      "let 1\n",
      "890 1\n",
      "exchange 1\n",
      "for 6\n",
      "Facebook, 2\n",
      "where 1\n",
      "free 1\n",
      "small 1\n",
      "applications 6\n",
      "developers 9\n",
      "First, 1\n",
      "show 2\n",
      "ranging 1\n",
      "they 2\n",
      "discover 1\n",
      "by 4\n",
      "2013 1\n",
      "(APIs) 1\n",
      "WhatsApp. 1\n",
      "environment, 1\n",
      "send 1\n",
      "accessed 1\n",
      "drove 1\n",
      "products, 1\n",
      "advertising 1\n",
      "learn 1\n",
      "each 2\n",
      "communicate 1\n",
      "interests. 1\n",
      "are 2\n",
      "we 4\n",
      "use 4\n",
      "Messenger. 1\n",
      "kinds 1\n",
      "from 11\n",
      "response, 1\n",
      "mobile 13\n",
      "number 1\n",
      "mobile-to-mobile 1\n",
      ". 1\n",
      "also 5\n",
      "Network. 1\n",
      "stay 1\n",
      "side 1\n",
      "videos, 2\n",
      "optimize 1\n",
      "had 2\n",
      "enables 2\n",
      "portion 1\n",
      "on 19\n",
      "developers’ 1\n",
      "Windows 2\n",
      "or 5\n",
      "The 1\n",
      "budget, 1\n",
      "messaging 3\n",
      "connect, 1\n",
      "dimensions. 1\n",
      "users 1\n",
      "variety 1\n",
      "Second, 1\n",
      "BlackBerry, 1\n",
      "allows 1\n",
      "measure 1\n",
      "iOS, 1\n",
      "that 6\n",
      "average 2\n",
      "choose 1\n",
      "Who 1\n",
      "their 14\n",
      "Instagram 2\n",
      "18% 1\n",
      "easily 1\n",
      "devices 3\n",
      "specify 1\n",
      "understand 1\n",
      "of 13\n",
      "When 1\n",
      "results. 1\n",
      "make 2\n",
      "only 1\n",
      "photo 1\n"
     ]
    }
   ],
   "source": [
    "for word in word_counts:\n",
    "    print(word, word_counts[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the extra credit, we can use *sorted* to sort the words. To sort a dictionary, we need to tell it to sort the keys based on their associated values; to sort from highest to lowest (instead of the default low-high order, we set the *reverse* parameter to True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and 42\n",
      "to 33\n",
      "on 19\n",
      "their 14\n",
      "Facebook 13\n",
      "mobile 13\n",
      "of 13\n",
      "the 12\n",
      "people 12\n",
      "from 11\n",
      "We 10\n",
      "a 9\n",
      "ads 9\n",
      "developers 9\n",
      "in 8\n",
      "with 8\n",
      "is 8\n",
      "our 7\n",
      "application 7\n",
      "them 6\n",
      "help 6\n",
      "who 6\n",
      "ad 6\n",
      "personal 6\n",
      "for 6\n",
      "applications 6\n",
      "that 6\n",
      "Our 5\n",
      "computers. 5\n",
      "enable 5\n",
      "an 5\n",
      "also 5\n",
      "or 5\n",
      "marketers 4\n",
      "December 4\n",
      "can 4\n",
      "as 4\n",
      "online 4\n",
      "share 4\n",
      "revenue 4\n",
      "web 4\n",
      "by 4\n",
      "we 4\n",
      "use 4\n",
      "including 3\n",
      "Create 3\n",
      "How 3\n",
      "available 3\n",
      "Messenger 3\n",
      "such 3\n",
      "Audience 3\n",
      "generate 3\n",
      "providing 3\n",
      "Value 3\n",
      "business 3\n",
      "Marketers 3\n",
      "increase 3\n",
      "monetize 3\n",
      "devices. 3\n",
      "other 3\n",
      "tools 3\n",
      "messaging 3\n",
      "devices 3\n",
      "Android, 2\n",
      "receive 2\n",
      "within 2\n",
      ", 2\n",
      "focuses 2\n",
      "photos 2\n",
      "compared 2\n",
      "marketers, 2\n",
      "million 2\n",
      "based 2\n",
      "developers. 2\n",
      "value 2\n",
      "Network, 2\n",
      "Payments 2\n",
      "those 2\n",
      "infrastructure 2\n",
      "sales, 2\n",
      "in-store 2\n",
      "purchase 2\n",
      "insights 2\n",
      "2014 2\n",
      "friends 2\n",
      "world 2\n",
      "applications. 2\n",
      "create 2\n",
      "reach 2\n",
      "advertisers 2\n",
      "Facebook, 2\n",
      "show 2\n",
      "they 2\n",
      "each 2\n",
      "are 2\n",
      "videos, 2\n",
      "had 2\n",
      "enables 2\n",
      "Windows 2\n",
      "average 2\n",
      "Instagram 2\n",
      "make 2\n",
      "DAUs 1\n",
      "website 1\n",
      "device 1\n",
      "driving 1\n",
      "objectives, 1\n",
      "types 1\n",
      "whether 1\n",
      "WhatsApp 1\n",
      "34% 1\n",
      "plugins, 1\n",
      "similarly 1\n",
      "designed 1\n",
      "what 1\n",
      "efforts 1\n",
      "Atlas, 1\n",
      "medium-sized 1\n",
      "programming 1\n",
      "right-hand 1\n",
      "everywhere 1\n",
      "computers, 1\n",
      "discovery 1\n",
      "payments 1\n",
      "trusted 1\n",
      "products 1\n",
      "platform's 1\n",
      "campaigns. 1\n",
      "connect 1\n",
      "others 1\n",
      "Facebook. 1\n",
      "active 1\n",
      "open 1\n",
      "set 1\n",
      "improve 1\n",
      "showing 1\n",
      "objectives 1\n",
      "Phone 1\n",
      "engaging 1\n",
      "places 1\n",
      "then 1\n",
      "top 1\n",
      "Finally, 1\n",
      "exposure, 1\n",
      "Instagram. 1\n",
      "goods 1\n",
      "Developers 1\n",
      "creating 1\n",
      "drive 1\n",
      "websites 1\n",
      "social 1\n",
      "at 1\n",
      "LiveRail. 1\n",
      "goals. 1\n",
      "about 1\n",
      "modifications 1\n",
      "grow 1\n",
      "performance 1\n",
      "(SMS) 1\n",
      "invites, 1\n",
      "throughout 1\n",
      "grow, 1\n",
      "mission 1\n",
      "connected. 1\n",
      "marketing 1\n",
      "chat 1\n",
      "By 1\n",
      "across 1\n",
      "but 1\n",
      "accessing 1\n",
      "connected 1\n",
      "distribution 1\n",
      "location, 1\n",
      "addition 1\n",
      "including: 1\n",
      "Use 1\n",
      "engagement. 1\n",
      "messages 1\n",
      "ads, 1\n",
      "opinions, 1\n",
      "not 1\n",
      "development 1\n",
      "brand. 1\n",
      "Feed 1\n",
      "appear 1\n",
      "want 1\n",
      "app 1\n",
      "them, 1\n",
      "share, 1\n",
      "followers 1\n",
      "In 1\n",
      "businesses, 1\n",
      "going 1\n",
      "requests, 1\n",
      "ideas, 1\n",
      "Facebook's 1\n",
      "have 1\n",
      "best 1\n",
      "how 1\n",
      "it 1\n",
      "customize 1\n",
      "supports 1\n",
      "awareness 1\n",
      "world. 1\n",
      "build 1\n",
      "serving 1\n",
      "sharing, 1\n",
      "News 1\n",
      "Phone, 1\n",
      "majority 1\n",
      "person 1\n",
      "tools, 1\n",
      "(DAUs) 1\n",
      "selling 1\n",
      "campaign 1\n",
      "secure, 1\n",
      "power 1\n",
      "align 1\n",
      "user 1\n",
      "sell 1\n",
      "feed 1\n",
      "well 1\n",
      "engagement 1\n",
      "build, 1\n",
      "integrates 1\n",
      "marketers. 1\n",
      "able 1\n",
      "using 1\n",
      "more 1\n",
      "multiple 1\n",
      "daily 1\n",
      "discover, 1\n",
      "integrate 1\n",
      "all 1\n",
      "provide 1\n",
      "745 1\n",
      "public 1\n",
      "filter 1\n",
      "through 1\n",
      "buy 1\n",
      "platforms 1\n",
      "digital 1\n",
      "audiences 1\n",
      "campaigns 1\n",
      "effects, 1\n",
      "virtual 1\n",
      "These 1\n",
      "2013. 1\n",
      "gender, 1\n",
      "instantly 1\n",
      "direct 1\n",
      "technology 1\n",
      "easy-to-use, 1\n",
      "around 1\n",
      "age, 1\n",
      "messaging, 1\n",
      "Overview 1\n",
      "factors 1\n",
      "achieve 1\n",
      "substantial 1\n",
      "brand, 1\n",
      "results 1\n",
      "people, 1\n",
      "useful 1\n",
      "interfaces 1\n",
      "closest 1\n",
      "priority 1\n",
      "reach. 1\n",
      "placements 1\n",
      "give 1\n",
      "take 1\n",
      "us, 1\n",
      "functionality 1\n",
      "both 1\n",
      "friends. 1\n",
      "texting 1\n",
      "activities 1\n",
      "large, 1\n",
      "works 1\n",
      "dynamically 1\n",
      "People 1\n",
      "iOS 1\n",
      "marketers' 1\n",
      "Nokia 1\n",
      "cross-platform 1\n",
      "directly 1\n",
      "seamlessly 1\n",
      "planning 1\n",
      "application. 1\n",
      "determines 1\n",
      "ways 1\n",
      "let 1\n",
      "890 1\n",
      "exchange 1\n",
      "where 1\n",
      "free 1\n",
      "small 1\n",
      "First, 1\n",
      "ranging 1\n",
      "discover 1\n",
      "2013 1\n",
      "(APIs) 1\n",
      "WhatsApp. 1\n",
      "environment, 1\n",
      "send 1\n",
      "accessed 1\n",
      "drove 1\n",
      "products, 1\n",
      "advertising 1\n",
      "learn 1\n",
      "communicate 1\n",
      "interests. 1\n",
      "Messenger. 1\n",
      "kinds 1\n",
      "response, 1\n",
      "number 1\n",
      "mobile-to-mobile 1\n",
      ". 1\n",
      "Network. 1\n",
      "stay 1\n",
      "side 1\n",
      "optimize 1\n",
      "portion 1\n",
      "developers’ 1\n",
      "The 1\n",
      "budget, 1\n",
      "connect, 1\n",
      "dimensions. 1\n",
      "users 1\n",
      "variety 1\n",
      "Second, 1\n",
      "BlackBerry, 1\n",
      "allows 1\n",
      "measure 1\n",
      "iOS, 1\n",
      "choose 1\n",
      "Who 1\n",
      "18% 1\n",
      "easily 1\n",
      "specify 1\n",
      "understand 1\n",
      "When 1\n",
      "results. 1\n",
      "only 1\n",
      "photo 1\n"
     ]
    }
   ],
   "source": [
    "for word in sorted(word_counts, key=lambda w: word_counts[w], reverse=True):\n",
    "    print(word, word_counts[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\n",
    "\n",
    "For this problem, I reload the data and add two steps before adding each word to the count dictionary: first I lower-case the word, then I use strip to remove any non-letter characters. Afterwards, I check whether it's a stop-word. If it isn't a stop-word, I proceed to add it to the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"data.txt\")\n",
    "word_counts = {}\n",
    "for row in f:\n",
    "    for word in row.split():\n",
    "        word = word.lower()\n",
    "        word = word.strip(\".,!?- \")\n",
    "        if word not in STOP_WORDS:\n",
    "            if word in word_counts:\n",
    "                word_counts[word] += 1\n",
    "            else:\n",
    "                word_counts[word] = 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more streamlined way is possible using the keyword *continue*: this skips everything below, and goes to the next iteration of the for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open(\"data.txt\")\n",
    "word_counts = {}\n",
    "for row in f:\n",
    "    for word in row.split():\n",
    "        word = word.lower()\n",
    "        word = word.strip(\".,!?- \")\n",
    "        if word in STOP_WORDS:\n",
    "            continue # Skip to the next word in the row\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can sort the words, this time storing them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_words = sorted(word_counts, key=lambda w: word_counts[w], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop over the sorted list, and print out those with the desired count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "facebook 16\n",
      "people 14\n",
      "mobile 13\n",
      "developers 12\n",
      "ads 10\n",
      "marketers 10\n",
      "application 8\n",
      "applications 8\n",
      "devices 6\n",
      "help 6\n",
      "computers 6\n",
      "personal 6\n",
      "ad 6\n",
      "share 5\n",
      "enable 5\n",
      "create 5\n",
      "value 5\n",
      "use 5\n",
      "also 5\n",
      "revenue 4\n",
      "web 4\n",
      "tools 4\n",
      "messenger 4\n",
      "online 4\n",
      "december 4\n",
      "messaging 4\n",
      " 3\n",
      "friends 3\n",
      "increase 3\n",
      "world 3\n",
      "including 3\n",
      "payments 3\n",
      "available 3\n",
      "monetize 3\n",
      "reach 3\n",
      "network 3\n",
      "instagram 3\n",
      "providing 3\n",
      "generate 3\n",
      "audience 3\n",
      "business 3\n",
      "engagement 2\n",
      "phone 2\n",
      "results 2\n",
      "windows 2\n",
      "receive 2\n",
      "insights 2\n",
      "within 2\n",
      "campaigns 2\n",
      "2014 2\n",
      "videos 2\n",
      "focuses 2\n",
      "connect 2\n",
      "whatsapp 2\n",
      "objectives 2\n",
      "compared 2\n",
      "photos 2\n",
      "million 2\n",
      "based 2\n",
      "advertisers 2\n",
      "ios 2\n",
      "brand 2\n",
      "infrastructure 2\n",
      "connected 2\n",
      "show 2\n",
      "discover 2\n",
      "2013 2\n",
      "android 2\n",
      "products 2\n",
      "sales 2\n",
      "enables 2\n",
      "grow 2\n",
      "build 2\n",
      "in-store 2\n",
      "purchase 2\n",
      "average 2\n",
      "make 2\n",
      "feed 2\n"
     ]
    }
   ],
   "source": [
    "for word in sorted_words:\n",
    "    count = word_counts[word]\n",
    "    if count > 1:\n",
    "        print(word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) \n",
    "\n",
    "Now we need to output the results. We create a file called, in this example, \"hw_3c.txt\", add headers, and repeat the loop above. To eliminate white space between the entries and commas, we concatenate them into a single string before writing to file. Don't forget that we also need to add a '\\n' to each line to make sure there's a line break in the text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"hw_3c.txt\", \"w\")\n",
    "header = \"Word,Count\\n\"\n",
    "f.write(header)\n",
    "\n",
    "for word in sorted_words:\n",
    "    count = str(word_counts[word]) # Convert the number to a string\n",
    "    row = word + \",\" + count + \"\\n\"\n",
    "    f.write(row)\n",
    "\n",
    "f.close() # Don't forget to close the file!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
